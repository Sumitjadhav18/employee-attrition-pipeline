{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a77a7e10-3891-45f7-a620-5c694039cdc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/16 12:42:53 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n2025/07/16 12:43:49 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: dbfs:/databricks/mlflow-tracking/3348170711752114/6f6d896a56da4b1d97827859693e7d0e/artifacts/attrition_model/sparkml, flavor: spark). Fall back to return ['pyspark==3.5.0']. Set logging level to DEBUG to see the full traceback. \n/databricks/python/lib/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logged to MLflow.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "# Load gold dataset\n",
    "df_gold = spark.read.format(\"delta\").load(\"/mnt/attrition/gold/hr_features/\")\n",
    "\n",
    "# Encode categorical features\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column + \"_index\") for column in [\n",
    "    \"BusinessTravel\", \"Department\", \"EducationField\", \"Gender\", \"JobRole\", \"MaritalStatus\", \"OverTime\"\n",
    "]]\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"Age\", \"DistanceFromHome\", \"Education\", \"EnvironmentSatisfaction\", \"MonthlyIncome\",\n",
    "        \"NumCompaniesWorked\", \"PercentSalaryHike\", \"TotalWorkingYears\", \"TrainingTimesLastYear\",\n",
    "        \"WorkLifeBalance\", \"YearsAtCompany\", \"YearsInCurrentRole\",\n",
    "        \"BusinessTravel_index\", \"Department_index\", \"EducationField_index\",\n",
    "        \"Gender_index\", \"JobRole_index\", \"MaritalStatus_index\", \"OverTime_index\"\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Define model\n",
    "lr = LogisticRegression(labelCol=\"Attrition\", featuresCol=\"features\")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=indexers + [assembler, lr])\n",
    "\n",
    "# Train model\n",
    "with mlflow.start_run():\n",
    "    model = pipeline.fit(df_gold)\n",
    "    mlflow.spark.log_model(model, \"attrition_model\")\n",
    "    mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
    "    print(\"Model logged to MLflow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f397685-55ca-4a21-958c-010498114a37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Metrics:\nAccuracy: 0.8544\nF1 Score: 0.8184\nPrecision: 0.8323\nRecall: 0.8544\nROC AUC: 0.7882\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "import mlflow\n",
    "\n",
    "# Step 1: Convert 'Attrition' (Yes/No) to numeric label (0/1)\n",
    "label_indexer = StringIndexer(inputCol=\"Attrition\", outputCol=\"label\")\n",
    "df_gold_indexed = label_indexer.fit(df_gold).transform(df_gold)\n",
    "\n",
    "# Step 2: Make predictions again using transformed data\n",
    "predictions = model.transform(df_gold_indexed)\n",
    "\n",
    "# Step 3: Define evaluators\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "precision_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
    ")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
    ")\n",
    "roc_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# Step 4: Evaluate and log metrics\n",
    "accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "f1_score = f1_evaluator.evaluate(predictions)\n",
    "precision = precision_evaluator.evaluate(predictions)\n",
    "recall = recall_evaluator.evaluate(predictions)\n",
    "roc_auc = roc_evaluator.evaluate(predictions)\n",
    "\n",
    "# Step 5: Log to MLflow\n",
    "mlflow.log_metric(\"accuracy\", accuracy)\n",
    "mlflow.log_metric(\"f1_score\", f1_score)\n",
    "mlflow.log_metric(\"precision\", precision)\n",
    "mlflow.log_metric(\"recall\", recall)\n",
    "mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "\n",
    "# Step 6: Print for report\n",
    "print(\"Model Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "5_ml_model_training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}